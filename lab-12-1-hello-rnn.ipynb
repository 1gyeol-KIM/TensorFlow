{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12-1 hello rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pprint import pprint\n",
    "tf.set_random_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepairing dataset\n",
    "hihell -> ihello, hell -> ello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'e', 2: 'h', 3: 'i', 4: 'l', 5: 'o'}\n",
      "{'<pad>': 0, 'e': 1, 'h': 2, 'i': 3, 'l': 4, 'o': 5}\n"
     ]
    }
   ],
   "source": [
    "strings = ['hihello', 'hello']\n",
    "char_set = ['<pad>'] + sorted(list(set('hihello' + 'hello')))\n",
    "idx2char = {idx : char for idx, char in enumerate(char_set)}\n",
    "char2idx = {char : idx for idx, char in enumerate(char_set)}\n",
    "\n",
    "x_strings = ['hihell', 'hell']\n",
    "y_strings = ['ihello', 'ello']\n",
    "\n",
    "# idx2char, char2idx 각각의 dictionary 확인\n",
    "print(idx2char)\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3 2 1 4 4 0 0]\n",
      " [2 1 4 4 0 0 0 0]]\n",
      "[[2 3 2 1 4 4 0 0]\n",
      " [2 1 4 4 0 0 0 0]]\n",
      "[6, 4]\n"
     ]
    }
   ],
   "source": [
    "x_data = list(map(lambda strings : [char2idx.get(char) for char in strings], x_strings))\n",
    "y_data = list(map(lambda strings : [char2idx.get(char) for char in strings], x_strings))\n",
    "x_data_len = list(map(lambda strings : len(strings), x_data))\n",
    "\n",
    "# padding\n",
    "max_length = 8\n",
    "x_data = pad_sequences(sequences = x_data, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "y_data = pad_sequences(sequences = y_data, maxlen = max_length, padding = 'post', truncating = 'post')\n",
    "\n",
    "# data 형태확인\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "print(x_data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "data = tf.data.Dataset.from_tensor_slices((x_data, y_data, x_data_len))\n",
    "data = data.shuffle(buffer_size = 2)\n",
    "data = data.batch(batch_size = 1)\n",
    "iterator = data.make_initializable_iterator()\n",
    "x, y, x_len = iterator.get_next()\n",
    "\n",
    "# hyper-parameters for lstm (many to many), one-hot encoding\n",
    "n_of_classes = len(idx2char)\n",
    "hidden_size = 10\n",
    "\n",
    "one_hot_encoding = tf.eye(num_rows = n_of_classes, dtype = tf.float32)\n",
    "one_hot_encoding = tf.get_variable(name = 'one_hot_encoding',\n",
    "                                   initializer = one_hot_encoding, trainable = False)\n",
    "\n",
    "x_batch = tf.nn.embedding_lookup(params = one_hot_encoding, ids = x)\n",
    "lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units = hidden_size, dtype = tf.float32)\n",
    "score_cell = tf.contrib.rnn.OutputProjectionWrapper(cell = lstm_cell, output_size = n_of_classes)\n",
    "outputs, _ = tf.nn.dynamic_rnn(cell = score_cell, inputs = x_batch,\n",
    "                               sequence_length = x_len, dtype = tf.float32)\n",
    "\n",
    "masking = tf.sequence_mask(lengths = x_len, maxlen = max_length, dtype = tf.float32)\n",
    "s2s_loss = tf.contrib.seq2seq.sequence_loss(logits = outputs, targets = y, weights = masking)\n",
    "\n",
    "prediction = tf.argmax(input = outputs, axis = -1)\n",
    "\n",
    "# training\n",
    "lr = .1\n",
    "opt = tf.train.AdamOptimizer(learning_rate = lr)\n",
    "training_op = opt.minimize(loss = s2s_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess = tf.Session(config = sess_config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :   1, loss : 1.619\n",
      "epoch :   2, loss : 1.086\n",
      "epoch :   3, loss : 0.796\n",
      "epoch :   4, loss : 0.462\n",
      "epoch :   5, loss : 0.278\n",
      "epoch :   6, loss : 0.144\n",
      "epoch :   7, loss : 0.072\n",
      "epoch :   8, loss : 0.040\n",
      "epoch :   9, loss : 0.022\n",
      "epoch :  10, loss : 0.012\n",
      "epoch :  11, loss : 0.007\n",
      "epoch :  12, loss : 0.004\n",
      "epoch :  13, loss : 0.003\n",
      "epoch :  14, loss : 0.002\n",
      "epoch :  15, loss : 0.002\n",
      "epoch :  16, loss : 0.001\n",
      "epoch :  17, loss : 0.001\n",
      "epoch :  18, loss : 0.001\n",
      "epoch :  19, loss : 0.001\n",
      "epoch :  20, loss : 0.001\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "tr_loss_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    sess.run(iterator.initializer)\n",
    "    avg_tr_loss = 0\n",
    "    step = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            _, loss = sess.run([training_op, s2s_loss])\n",
    "            avg_tr_loss += loss\n",
    "            step += 1\n",
    "    except:\n",
    "        avg_tr_loss /= step\n",
    "        tr_loss_hist.append(avg_tr_loss)\n",
    "        \n",
    "    print('epoch : {:3}, loss : {:.3f}'.format(epoch + 1, avg_tr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = sess.run(prediction, feed_dict = {x : x_data, x_len : x_data_len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true : [[2 3 2 1 4 4 0 0]\n",
      " [2 1 4 4 0 0 0 0]], prediction : [[2 3 2 1 4 4 0 0]\n",
      " [2 1 4 4 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print('true : {}, prediction : {}'.format(y_data, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
