{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06 Softmax Zoo_classifier-eager\n",
    "\n",
    "* Softmax를 사용하여 Zoo 데이터를 활용하여 분류를 진행합니다.\n",
    "\n",
    "### 기본 Library 선언 및 Tensorflow 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "tf.enable_eager_execution must be called at program startup.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-21c3b784a4e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_eager_execution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m777\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution\u001b[0;34m(config, device_policy, execution_mode)\u001b[0m\n\u001b[1;32m   5185\u001b[0m   \"\"\"\n\u001b[1;32m   5186\u001b[0m   return enable_eager_execution_internal(\n\u001b[0;32m-> 5187\u001b[0;31m       config, device_policy, execution_mode, None)\n\u001b[0m\u001b[1;32m   5188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36menable_eager_execution_internal\u001b[0;34m(config, device_policy, execution_mode, server_def)\u001b[0m\n\u001b[1;32m   5253\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5254\u001b[0m     raise ValueError(\n\u001b[0;32m-> 5255\u001b[0;31m         \"tf.enable_eager_execution must be called at program startup.\")\n\u001b[0m\u001b[1;32m   5256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5257\u001b[0m   \u001b[0;31m# Monkey patch to get rid of an unnecessary conditional since the context is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: tf.enable_eager_execution must be called at program startup."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 16) (101, 1)\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, y_data.shape)\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "# Make Y data as onehot shape\n",
    "Y_one_hot = tf.one_hot(list(y_data), nb_classes)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_data, Y_one_hot)).batch(16).repeat(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weight and bias setting\n",
    "W = tfe.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tfe.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "variables = [W, b]\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "def hypothesis(X):\n",
    "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "def cost_fn(X, Y):\n",
    "    logits = hypothesis(X)\n",
    "    cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=tf.stop_gradient(Y))\n",
    "    cost = tf.reduce_mean(cost_i)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def grad_fn(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = cost_fn(X, Y)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        return grads\n",
    "    \n",
    "def prediction(X, Y):\n",
    "    pred = tf.argmax(hypothesis(X), 1)\n",
    "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1 Loss: 1.4284837245941162, Acc: 0.7326732873916626\n",
      "Steps: 500 Loss: 1.4279112815856934, Acc: 0.7326732873916626\n",
      "Steps: 1000 Loss: 1.4274009466171265, Acc: 0.7326732873916626\n",
      "Steps: 1500 Loss: 1.4269442558288574, Acc: 0.7326732873916626\n",
      "Steps: 2000 Loss: 1.426532506942749, Acc: 0.7326732873916626\n",
      "Steps: 2500 Loss: 1.4261595010757446, Acc: 0.7326732873916626\n",
      "Steps: 3000 Loss: 1.4258196353912354, Acc: 0.7326732873916626\n",
      "Steps: 3500 Loss: 1.4255083799362183, Acc: 0.7326732873916626\n",
      "Steps: 4000 Loss: 1.425222396850586, Acc: 0.7326732873916626\n",
      "Steps: 4500 Loss: 1.4249582290649414, Acc: 0.7326732873916626\n",
      "Steps: 5000 Loss: 1.4247134923934937, Acc: 0.7326732873916626\n"
     ]
    }
   ],
   "source": [
    "def fit(X, Y, epochs=5000, verbose=500):\n",
    "    optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.05)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        grads = grad_fn(X, Y)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "        if (i==0) | ((i+1)%verbose==0):\n",
    "#             print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
    "            acc = prediction(X, Y).numpy()\n",
    "            loss = cost_fn(X, Y).numpy()\n",
    "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))\n",
    "\n",
    "fit(x_data, Y_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tfe.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "        self.b = tfe.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        # tf.nn.softmax computes softmax activations\n",
    "        # softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "        logits = tf.matmul(X, self.W) + self.b\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "    def loss_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=Y)\n",
    "        cost = tf.reduce_mean(cost_i)\n",
    "        \n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.loss_fn(X, Y)\n",
    "            grads = tape.gradient(loss, self.variables)\n",
    "            \n",
    "            return grads\n",
    "    \n",
    "    def prediction(self, X, Y):\n",
    "        pred = tf.argmax(self.softmax_regression(X), 1)\n",
    "        correct_prediction = tf.equal(pred, tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def fit(self, dataset, epochs=1001, verbose=50):\n",
    "        optimizer =  tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            step = 0\n",
    "            \n",
    "            for features, labels in tfe.Iterator(dataset):\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "                grads = self.grad_fn(features, labels)\n",
    "                optimizer.apply_gradients(zip(grads, self.variables))\n",
    "                \n",
    "                if step % verbose == 0:\n",
    "                    acc = self.prediction(features, labels).numpy()\n",
    "                    loss = self.loss_fn(features, labels).numpy()\n",
    "                    print('Steps: {} Loss: {}, Acc: {}'.format(step, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 50 Loss: 2.003817558288574, Acc: 0.1875\n",
      "Steps: 100 Loss: 2.1641604900360107, Acc: 0.0\n",
      "Steps: 150 Loss: 2.0985264778137207, Acc: 0.0625\n",
      "Steps: 200 Loss: 2.098926305770874, Acc: 0.0625\n",
      "Steps: 250 Loss: 2.0350992679595947, Acc: 0.125\n",
      "Steps: 300 Loss: 2.0418856143951416, Acc: 0.125\n",
      "Steps: 350 Loss: 1.9834251403808594, Acc: 0.20000000298023224\n",
      "Steps: 400 Loss: 1.9944957494735718, Acc: 0.1875\n",
      "Steps: 450 Loss: 2.1639621257781982, Acc: 0.0\n",
      "Steps: 500 Loss: 2.0972094535827637, Acc: 0.0625\n",
      "Steps: 550 Loss: 2.0975914001464844, Acc: 0.0625\n",
      "Steps: 600 Loss: 2.022022247314453, Acc: 0.1875\n",
      "Steps: 650 Loss: 2.0380849838256836, Acc: 0.125\n",
      "Steps: 700 Loss: 1.9773708581924438, Acc: 0.20000000298023224\n",
      "Steps: 50 Loss: 1.9882460832595825, Acc: 0.1875\n",
      "Steps: 100 Loss: 2.1633596420288086, Acc: 0.0\n",
      "Steps: 150 Loss: 2.096513271331787, Acc: 0.0625\n",
      "Steps: 200 Loss: 2.0957467555999756, Acc: 0.0625\n",
      "Steps: 250 Loss: 2.009125232696533, Acc: 0.1875\n",
      "Steps: 300 Loss: 2.035257339477539, Acc: 0.125\n",
      "Steps: 350 Loss: 1.9740378856658936, Acc: 0.20000000298023224\n",
      "Steps: 400 Loss: 1.98466956615448, Acc: 0.1875\n",
      "Steps: 450 Loss: 2.1626508235931396, Acc: 0.0\n",
      "Steps: 500 Loss: 2.0966885089874268, Acc: 0.0625\n",
      "Steps: 550 Loss: 2.09409761428833, Acc: 0.0625\n",
      "Steps: 600 Loss: 1.9999327659606934, Acc: 0.1875\n",
      "Steps: 650 Loss: 2.0330543518066406, Acc: 0.125\n",
      "Steps: 700 Loss: 1.971970796585083, Acc: 0.20000000298023224\n",
      "Steps: 50 Loss: 1.982414960861206, Acc: 0.1875\n",
      "Steps: 100 Loss: 2.1619510650634766, Acc: 0.0\n",
      "Steps: 150 Loss: 2.0969998836517334, Acc: 0.0625\n",
      "Steps: 200 Loss: 2.0922634601593018, Acc: 0.0625\n",
      "Steps: 250 Loss: 1.993890643119812, Acc: 0.1875\n",
      "Steps: 300 Loss: 2.0309839248657227, Acc: 0.125\n",
      "Steps: 350 Loss: 1.970636010169983, Acc: 0.20000000298023224\n",
      "Steps: 400 Loss: 1.9810478687286377, Acc: 0.1875\n",
      "Steps: 450 Loss: 2.1610159873962402, Acc: 0.0\n",
      "Steps: 500 Loss: 2.0972371101379395, Acc: 0.0625\n",
      "Steps: 550 Loss: 2.0892369747161865, Acc: 0.0625\n",
      "Steps: 600 Loss: 1.9895262718200684, Acc: 0.1875\n",
      "Steps: 650 Loss: 2.0279700756073, Acc: 0.125\n",
      "Steps: 700 Loss: 1.9696204662322998, Acc: 0.20000000298023224\n",
      "Steps: 50 Loss: 1.9808892011642456, Acc: 0.1875\n",
      "Steps: 100 Loss: 2.158811092376709, Acc: 0.0\n",
      "Steps: 150 Loss: 2.0971031188964844, Acc: 0.0625\n",
      "Steps: 200 Loss: 2.080667018890381, Acc: 0.0625\n",
      "Steps: 250 Loss: 1.9848716259002686, Acc: 0.1875\n",
      "Steps: 300 Loss: 2.016366481781006, Acc: 0.125\n",
      "Steps: 350 Loss: 1.9647051095962524, Acc: 0.20000000298023224\n",
      "Steps: 400 Loss: 1.9684069156646729, Acc: 0.1875\n",
      "Steps: 450 Loss: 1.7450400590896606, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7883522510528564, Acc: 0.375\n",
      "Steps: 550 Loss: 1.7331265211105347, Acc: 0.4375\n",
      "Steps: 600 Loss: 1.5412209033966064, Acc: 0.625\n",
      "Steps: 650 Loss: 1.7869658470153809, Acc: 0.375\n",
      "Steps: 700 Loss: 1.5626487731933594, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.5490514039993286, Acc: 0.625\n",
      "Steps: 100 Loss: 1.7127301692962646, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7270097732543945, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.699645757675171, Acc: 0.5\n",
      "Steps: 250 Loss: 1.4940311908721924, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.7200196981430054, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5672996044158936, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.4974730014801025, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.711233377456665, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7180156707763672, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.6761760711669922, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4827606678009033, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.7028155326843262, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5657209157943726, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4906234741210938, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.7107876539230347, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.71640145778656, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.6685657501220703, Acc: 0.5\n",
      "Steps: 250 Loss: 1.4781256914138794, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6934916973114014, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5639266967773438, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.4866600036621094, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7103921175003052, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7153818607330322, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.6644773483276367, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4754204750061035, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6874852180480957, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5623056888580322, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4840320348739624, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.7099463939666748, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7144522666931152, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.6618385314941406, Acc: 0.5\n",
      "Steps: 250 Loss: 1.4737319946289062, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6831408739089966, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5608446598052979, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.482175350189209, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7093993425369263, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7134673595428467, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.6600308418273926, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4726126194000244, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6797107458114624, Acc: 0.5\n",
      "Steps: 700 Loss: 1.559389352798462, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4807789325714111, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.708669662475586, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7123138904571533, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.6587613821029663, Acc: 0.5\n",
      "Steps: 250 Loss: 1.4718245267868042, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6768138408660889, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5577123165130615, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.4796048402786255, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7076681852340698, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7109401226043701, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.6577978134155273, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4712553024291992, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6742080450057983, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5556119680404663, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4783644676208496, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.7063766717910767, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.709421157836914, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.6568375825881958, Acc: 0.5\n",
      "Steps: 250 Loss: 1.4708867073059082, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6717238426208496, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5532095432281494, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.4766817092895508, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7049627304077148, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.707850694656372, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.6556028127670288, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4707221984863281, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6693072319030762, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5510411262512207, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4743785858154297, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.703736424446106, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7062745094299316, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.654195785522461, Acc: 0.5\n",
      "Steps: 250 Loss: 1.470638632774353, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.66703200340271, Acc: 0.5\n",
      "Steps: 350 Loss: 1.54953134059906, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.471922755241394, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7029225826263428, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7048931121826172, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.652982234954834, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4704118967056274, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6649646759033203, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5486910343170166, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4700297117233276, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.7024509906768799, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7039061784744263, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.6520541906356812, Acc: 0.5\n",
      "Steps: 250 Loss: 1.469984531402588, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6630878448486328, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5482723712921143, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.4688286781311035, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7020964622497559, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7032625675201416, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.651319980621338, Acc: 0.5\n",
      "Steps: 600 Loss: 1.469468116760254, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.661372423171997, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5480420589447021, Acc: 0.6000000238418579\n",
      "Steps: 50 Loss: 1.4680906534194946, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.7016898393630981, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7028201818466187, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.650726318359375, Acc: 0.5\n",
      "Steps: 250 Loss: 1.4689621925354004, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.659745454788208, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5478761196136475, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.467646598815918, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.7010283470153809, Acc: 0.4375\n",
      "Steps: 500 Loss: 1.7024664878845215, Acc: 0.4375\n",
      "Steps: 550 Loss: 1.6502559185028076, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4685070514678955, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.657913327217102, Acc: 0.5\n",
      "Steps: 700 Loss: 1.5477030277252197, Acc: 0.6000000238418579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 50 Loss: 1.4675233364105225, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.6991726160049438, Acc: 0.4375\n",
      "Steps: 150 Loss: 1.7020220756530762, Acc: 0.4375\n",
      "Steps: 200 Loss: 1.6499699354171753, Acc: 0.5\n",
      "Steps: 250 Loss: 1.468111515045166, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.652348279953003, Acc: 0.5\n",
      "Steps: 350 Loss: 1.5467439889907837, Acc: 0.6000000238418579\n",
      "Steps: 400 Loss: 1.4723237752914429, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.635450839996338, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.6394238471984863, Acc: 0.5\n",
      "Steps: 550 Loss: 1.652381181716919, Acc: 0.5\n",
      "Steps: 600 Loss: 1.4679474830627441, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.628553032875061, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.391684651374817, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.5001506805419922, Acc: 0.625\n",
      "Steps: 100 Loss: 1.588355541229248, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5706688165664673, Acc: 0.625\n",
      "Steps: 200 Loss: 1.6253489255905151, Acc: 0.5\n",
      "Steps: 250 Loss: 1.46742844581604, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6105856895446777, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3818126916885376, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.477599024772644, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5890692472457886, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.550733208656311, Acc: 0.625\n",
      "Steps: 550 Loss: 1.6059658527374268, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4663288593292236, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6061385869979858, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3742375373840332, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4728446006774902, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5881366729736328, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5427608489990234, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5991809368133545, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4656341075897217, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6035387516021729, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3708841800689697, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4708532094955444, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5874483585357666, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5385328531265259, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5959274768829346, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4651813507080078, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.6016879081726074, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3689006567001343, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4697023630142212, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5869433879852295, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.535865068435669, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5940334796905518, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.464850902557373, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.6002382040023804, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.367480993270874, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4689255952835083, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5865412950515747, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5340030193328857, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5927884578704834, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4645897150039673, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5990444421768188, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3663594722747803, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4683525562286377, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5861995220184326, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5326147079467773, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5918993949890137, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4643728733062744, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5980318784713745, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3654277324676514, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.467904806137085, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.585896372795105, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5315301418304443, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5912251472473145, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4641876220703125, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.597156047821045, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.364630937576294, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4675403833389282, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5856199264526367, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5306525230407715, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5906904935836792, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4640262126922607, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5963873863220215, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3639370203018188, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4672343730926514, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5853639841079712, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5299229621887207, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5902518033981323, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4638839960098267, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.595705270767212, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3633252382278442, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4669716358184814, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5851247310638428, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5293030738830566, Acc: 0.625\n",
      "Steps: 200 Loss: 1.589881420135498, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.463757038116455, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5950947999954224, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.362779974937439, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4667418003082275, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5848995447158813, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5287666320800781, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5895617008209229, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4636428356170654, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5945440530776978, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.362290382385254, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4665377140045166, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5846872329711914, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5282957553863525, Acc: 0.625\n",
      "Steps: 200 Loss: 1.589280605316162, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4635392427444458, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5940442085266113, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.361846923828125, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4663543701171875, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5844863653182983, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5278770923614502, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5890294313430786, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4634448289871216, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.59358811378479, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.361443281173706, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4661877155303955, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5842961072921753, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5275006294250488, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5888019800186157, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.463357925415039, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.593169927597046, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3610732555389404, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4660351276397705, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5841158628463745, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5271592140197754, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5885937213897705, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.46327805519104, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5927848815917969, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3607323169708252, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.465894103050232, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5839447975158691, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5268466472625732, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5884010791778564, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4632039070129395, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5924286842346191, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.360416293144226, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4657632112503052, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5837821960449219, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5265586376190186, Acc: 0.625\n",
      "Steps: 550 Loss: 1.588221549987793, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4631351232528687, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5920982360839844, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3601219654083252, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4656407833099365, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5836275815963745, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5262912511825562, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5880526304244995, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4630711078643799, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5917904376983643, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3598464727401733, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4655259847640991, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5834801197052002, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5260417461395264, Acc: 0.625\n",
      "Steps: 550 Loss: 1.587892770767212, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4630110263824463, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5915030241012573, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.359587550163269, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.465417504310608, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5833393335342407, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5258073806762695, Acc: 0.625\n",
      "Steps: 200 Loss: 1.587740421295166, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4629552364349365, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.591233730316162, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3593428134918213, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4653148651123047, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5832043886184692, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5255862474441528, Acc: 0.625\n",
      "Steps: 550 Loss: 1.587594747543335, Acc: 0.5625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 600 Loss: 1.4629029035568237, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5909807682037354, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3591105937957764, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.465217113494873, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5830748081207275, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.525376319885254, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5874543190002441, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4628537893295288, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.5907427072525024, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3588892221450806, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4651236534118652, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5829501152038574, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5251762866973877, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5873186588287354, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.4628077745437622, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5905179977416992, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3586770296096802, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4650342464447021, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.582829475402832, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.5249848365783691, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5871868133544922, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.4627646207809448, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.590305209159851, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3584728240966797, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4649484157562256, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5827127695083618, Acc: 0.5625\n",
      "Steps: 500 Loss: 1.5248007774353027, Acc: 0.625\n",
      "Steps: 550 Loss: 1.5870579481124878, Acc: 0.5625\n",
      "Steps: 600 Loss: 1.462724208831787, Acc: 0.6875\n",
      "Steps: 650 Loss: 1.5901035070419312, Acc: 0.5625\n",
      "Steps: 700 Loss: 1.3582756519317627, Acc: 0.800000011920929\n",
      "Steps: 50 Loss: 1.4648652076721191, Acc: 0.6875\n",
      "Steps: 100 Loss: 1.5825992822647095, Acc: 0.5625\n",
      "Steps: 150 Loss: 1.524623155593872, Acc: 0.625\n",
      "Steps: 200 Loss: 1.5869317054748535, Acc: 0.5625\n",
      "Steps: 250 Loss: 1.46268630027771, Acc: 0.6875\n",
      "Steps: 300 Loss: 1.589911937713623, Acc: 0.5625\n",
      "Steps: 350 Loss: 1.3580843210220337, Acc: 0.800000011920929\n",
      "Steps: 400 Loss: 1.4647847414016724, Acc: 0.6875\n",
      "Steps: 450 Loss: 1.5824884176254272, Acc: 0.5625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9f13537e903e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_classifer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-1eb6164948bb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, epochs, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-1eb6164948bb>\u001b[0m in \u001b[0;36mgrad_fn\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m    856\u001b[0m     flat_grad = imperative_grad.imperative_grad(\n\u001b[1;32m    857\u001b[0m         \u001b[0m_default_vspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m         output_gradients=output_gradients)\n\u001b[0m\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(vspace, tape, target, sources, output_gradients)\u001b[0m\n\u001b[1;32m     61\u001b[0m   \"\"\"\n\u001b[1;32m     62\u001b[0m   return pywrap_tensorflow.TFE_Py_TapeGradient(\n\u001b[0;32m---> 63\u001b[0;31m       tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n\u001b[0m",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tf110/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_SoftmaxCrossEntropyWithLogitsGrad\u001b[0;34m(op, grad_loss, grad_grad)\u001b[0m\n\u001b[1;32m    479\u001b[0m              softmax)\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BroadcastMul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
